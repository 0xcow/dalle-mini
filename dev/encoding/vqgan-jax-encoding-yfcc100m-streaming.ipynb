{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0b72877",
   "metadata": {},
   "source": [
    "# vqgan-jax-encoding-yfcc100m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7b31e6",
   "metadata": {},
   "source": [
    "Encoding notebook for YFCC100M.\n",
    "\n",
    "This dataset was prepared by @borisdayma in Json lines format. We'll load it in streaming mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b59489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import os\n",
    "\n",
    "import jax\n",
    "from jax import pmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c4c1e6",
   "metadata": {},
   "source": [
    "## Dataset and Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e900d15",
   "metadata": {},
   "source": [
    "This is a local test on a small subset stored in disk. It can also be used in streaming mode, so it is useful to validate how everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33861477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81b19eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "yfcc100m = Path('/home/pedro/data/YFCC100M_OpenAI_subset')\n",
    "# Images are 'sharded' from the following directory\n",
    "yfcc100m_images = yfcc100m/'data'/'images'\n",
    "yfcc100m_metadata = yfcc100m/'metadata_existing.jsonl'\n",
    "yfcc100m_output = yfcc100m/'metadata_encoded.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e7b71c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128     # Per device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7811648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d451e6",
   "metadata": {},
   "source": [
    "We load the dataset in streaming mode. This allows us to process the whole OpenAI subset of YFCC100M without having to download a local copy in the TPU.\n",
    "\n",
    "Datasets loaded in streaming mode are iterables and have no `len`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73bf5abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d6917ade73efa578\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('json', data_files=str(yfcc100m_metadata), streaming=True)\n",
    "dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e616a56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 12.0,\n",
       " 'capturedevice': '',\n",
       " 'datetaken': '2004-09-01 15:21:46.0',\n",
       " 'dateuploaded': '1094077306',\n",
       " 'description': 'The+door+we+climbed+through+to+the+cafeteria',\n",
       " 'description_clean': 'The door we climbed through to the cafeteria',\n",
       " 'downloadurl': 'http://farm1.staticflickr.com/1/317823_1b42b71779.jpg',\n",
       " 'ext': 'jpg',\n",
       " 'farmid': 1,\n",
       " 'key': '10752f5dcc9b9ca5309542708b1bacf',\n",
       " 'latitude': 51.897893,\n",
       " 'licensename': 'Attribution-NonCommercial-NoDerivs License',\n",
       " 'licenseurl': 'http://creativecommons.org/licenses/by-nc-nd/2.0/',\n",
       " 'longitude': -8.506336,\n",
       " 'machinetags': '',\n",
       " 'marker': 0,\n",
       " 'pageurl': 'http://www.flickr.com/photos/51035594319@N01/317823/',\n",
       " 'photoid': 317823,\n",
       " 'secret': '1b42b71779',\n",
       " 'secretoriginal': '1b42b71779',\n",
       " 'serverid': 1,\n",
       " 'title': 'st+annes-18',\n",
       " 'title_clean': 'st annes-18',\n",
       " 'uid': '51035594319@N01',\n",
       " 'unickname': 'twelves',\n",
       " 'usertags': 'abandoned,asylum,cork,door,ireland,urban+decay,urban+exploration'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0793c26a",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86415769",
   "metadata": {},
   "source": [
    "* Images\n",
    "\n",
    "We retrieve them based on their `key`, then we transform them so they are center-cropped and square, all of the same size so we can build batches for TPU/GPU processing.\n",
    "\n",
    "* Captions: we extract a single `caption` column from the source data, by concatenating the cleaned title and description.\n",
    "\n",
    "These transformations are done using the Datasets `map` function. In the case of streaming datasets, transformations will run as needed instead of pre-processing the dataset at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2f0bcf",
   "metadata": {},
   "source": [
    "The following function retrieves an image based on its `key`. In my tests I'll read it from the filesystem. In the final dataset, it will be downloaded remotely from the appropriate zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd2cc401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_data(path, key, ext):\n",
    "    image_path = (path/key[0:3]/key[3:6]/key).with_suffix(\".\" + ext)\n",
    "    return Image.open(image_path).convert('RGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e5887",
   "metadata": {},
   "source": [
    "This function does the center-cropping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e73dfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop(image, max_size=256):\n",
    "    s = min(image.size)\n",
    "\n",
    "    # Note: we allow upscaling too. We should exclude small images.\n",
    "    r = max_size / s\n",
    "    s = (round(r * image.size[1]), round(r * image.size[0]))\n",
    "    image = TF.resize(image, s, interpolation=InterpolationMode.LANCZOS)\n",
    "    image = TF.center_crop(image, output_size=2 * [max_size])\n",
    "    image = torch.from_numpy(np.array(image, copy=True))\n",
    "    image = torch.unsqueeze(image, 0)\n",
    "    return image.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4522b9",
   "metadata": {},
   "source": [
    "And this is the basic transformation function to use in `map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2566ff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(examples):\n",
    "    result = {'key': [], 'caption': [], 'image': []}\n",
    "\n",
    "    for key, ext, title, description in zip(examples['key'], examples['ext'], examples['title_clean'], examples['description_clean']):\n",
    "        image = get_image_data(yfcc100m_images, key, ext)\n",
    "        image = center_crop(image)\n",
    "        \n",
    "        caption = f\"{title}. {description}\"\n",
    "        \n",
    "        result['key'].append(key)\n",
    "        result['caption'].append(caption)\n",
    "        result['image'].append(image)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e519e475",
   "metadata": {},
   "source": [
    "Unlike when using non-streaming datasets, the following operation completes immediately in streaming mode. Samples will be processed as needed. We use a `batch_size` of the same size as the processing size to read as many items we'll consume in a dataloader step. We could use a different size, this way we do a retrieval per batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10d7750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_dataset = dataset.map(prepare_data, batched=True, batch_size=batch_size * jax.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8595539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.76 s, sys: 0 ns, total: 8.76 s\n",
      "Wall time: 8.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = next(iter(prepared_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b808e5c",
   "metadata": {},
   "source": [
    "We have a problem here. Our `prepare_data` function receives a batch, but it loads and transforms images sequentially. We'll now try to retrieve images in parallel, using a simple `Pool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebc316e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8309505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(key):\n",
    "    image = get_image_data(yfcc100m_images, key, 'jpg')\n",
    "    image = center_crop(image)\n",
    "    return image\n",
    "\n",
    "# Create a single pool that will be reused\n",
    "pool = Pool(16)\n",
    "\n",
    "def parallel_prepare_data(examples):\n",
    "    # Retrieve images in parallel using the global pool\n",
    "    keys = examples['key']\n",
    "    images = pool.map(get_image, keys)\n",
    "    captions = [f\"{title}. {description}\" for (title, description) in zip(examples['title_clean'], examples['description_clean'])]\n",
    "\n",
    "    result = {'key': keys, 'caption': captions, 'image': images}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3130cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_dataset = dataset.map(parallel_prepare_data, batched=True, batch_size=batch_size * jax.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7964380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 404 ms, sys: 704 ms, total: 1.11 s\n",
      "Wall time: 1.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = next(iter(prepared_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ccb668",
   "metadata": {},
   "source": [
    "We'll use this method for encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d50a51",
   "metadata": {},
   "source": [
    "### Torch DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a4bbc6",
   "metadata": {},
   "source": [
    "We'll create a PyTorch DataLoader for convenience. This allows us to easily take batches of our desired size.\n",
    "\n",
    "We won't be using parallel processing of the DataLoader for now, as the items will be retrieved on the fly. We could attempt to do it using these recommendations: https://pytorch.org/docs/stable/data.html#multi-process-data-loading. For now, we'll just leverage our parallel image loading method and retrieve batches sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1c08b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a296677",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset = prepared_dataset.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f28cb96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.iterable_dataset.iterable_dataset.<locals>.TorchIterableDataset"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(torch_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2df5e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_loader = DataLoader(torch_dataset, batch_size=batch_size * jax.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a45061eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(torch_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71d027fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 1, 256, 256, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['image'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a354472b",
   "metadata": {},
   "source": [
    "## VQGAN-JAX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fcf01d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vqgan_jax.modeling_flax_vqgan import VQModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daa636d",
   "metadata": {},
   "source": [
    "We'll use a VQGAN trained with Taming Transformers and converted to a JAX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47a8b818",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n"
     ]
    }
   ],
   "source": [
    "model = VQModel.from_pretrained(\"flax-community/vqgan_f16_16384\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad01c3",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6686b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training.common_utils import shard\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "322a4619",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.pmap, axis_name=\"batch\")\n",
    "def encode(batch):\n",
    "    # Not sure if we should `replicate` params, does not seem to have any effect\n",
    "    _, indices = model.encode(batch)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14375a41",
   "metadata": {},
   "source": [
    "### Putting it all together in a encoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2210705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def encode_captioned_dataset(dataset, prepare_function, output_tsv, batch_size=32):\n",
    "    \"\"\"\n",
    "    :param dataset: Streaming Dataset with source data.\n",
    "    :param prepare_function: Data preparation function, to be applied to the dataset using `map`.\n",
    "    :param output_tsv: Destination file. Must not exist.\n",
    "    :batch_size: Per-device batch size\n",
    "    \"\"\"\n",
    "    if os.path.isfile(output_tsv):\n",
    "        print(f\"Destination file {output_tsv} already exists, please move away.\")\n",
    "        return\n",
    "    \n",
    "    num_tpus = jax.device_count()\n",
    "    \n",
    "    prepared_dataset = dataset.map(prepare_function, batched=True, batch_size=batch_size * num_tpus)\n",
    "    torch_dataset = prepared_dataset.with_format(\"torch\")\n",
    "    dataloader = DataLoader(torch_dataset, batch_size=batch_size * num_tpus)\n",
    "\n",
    "    # We save each batch to avoid reallocation of buffers as we process them.\n",
    "    # We keep the file open to prevent excessive file seeks.\n",
    "    # TODO: save to .jsonl, create a new file every half million images or so\n",
    "    with open(output_tsv, \"w\") as file:\n",
    "        iter_loader = iter(dataloader)\n",
    "        for batch in tqdm(iter_loader):\n",
    "            try:\n",
    "                images = batch[\"image\"].numpy()\n",
    "                images = shard(images.squeeze())\n",
    "                encoded = encode(images)\n",
    "                encoded = encoded.reshape(-1, encoded.shape[-1])\n",
    "\n",
    "                # Extract captions from the dataset\n",
    "                keys = batch[\"key\"]\n",
    "                captions = batch[\"caption\"]\n",
    "                encoded_as_string = list(map(lambda item: np.array2string(item, separator=',', max_line_width=50000, formatter={'int':lambda x: str(x)}), encoded))\n",
    "                batch_df = pd.DataFrame.from_dict({\"key\": keys, \"caption\": captions, \"encoding\": encoded_as_string})\n",
    "                batch_df.to_csv(file, sep='\\t', header=False, index=None)\n",
    "            except ValueError:\n",
    "                # Ignore incomplete last batch, which cannot be sharded\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7704863d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57it [01:50,  1.93s/it]\n"
     ]
    }
   ],
   "source": [
    "encode_captioned_dataset(dataset, parallel_prepare_data, yfcc100m_output, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e266a70a",
   "metadata": {},
   "source": [
    "This is not as efficient as a regular dataset loaded with multiple workers, but it's not bad. Non-streaming parallel processing took ~1:30 in a similar test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8953dd84",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db471c52d602b4f5f40ecaf278e88ccfef85c29d0a1a07185b0d51fc7acf4e26"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
