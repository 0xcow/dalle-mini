{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0b72877",
   "metadata": {},
   "source": [
    "# vqgan-jax-encoding-yfcc100m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7b31e6",
   "metadata": {},
   "source": [
    "Encoding notebook for YFCC100M.\n",
    "\n",
    "This dataset was prepared by @borisdayma in Json lines format. This version assumes that all images are available in the filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b59489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets.folder import default_loader\n",
    "import os\n",
    "\n",
    "import jax\n",
    "from jax import pmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbbbf43",
   "metadata": {},
   "source": [
    "Verify that JAX is seeing our devices correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85ae8ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
       " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
       " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
       " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
       " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
       " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
       " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
       " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c4c1e6",
   "metadata": {},
   "source": [
    "## Dataset and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33861477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81b19eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "yfcc100m = Path('/yfcc')\n",
    "# Images are 'sharded' from the following directory\n",
    "yfcc100m_images = yfcc100m/'data'/'images'\n",
    "yfcc100m_metadata = yfcc100m/'metadata_YFCC100M.jsonl'\n",
    "yfcc100m_filtered = yfcc100m/'metadata_filtered.jsonl'\n",
    "yfcc100m_output = yfcc100m/'encoded'      # Output directory for encoded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10e7c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128      # Per device\n",
    "num_workers = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7811648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc26bdcd",
   "metadata": {},
   "source": [
    "### Cleanup and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d3bc9",
   "metadata": {},
   "source": [
    "Filter out entries referring to very small images or inexistent ones. This may not be necessary when using the final version of the dataset.\n",
    "\n",
    "We also remove columns we are not interested in, and create a new `caption` column concatenating clean title and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d58f4623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_is_valid(path, key, ext='jpg'):\n",
    "    image_path = (path/key[0:3]/key[3:6]/key).with_suffix(\".\" + ext)\n",
    "    try:\n",
    "        # Exclude small files. We should ideally filter them based on pixel size, but that requires decoding them all.\n",
    "        return image_path.lstat().st_size >= 16384\n",
    "    except:\n",
    "        return False   # Does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f8184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def create_caption(title, description):\n",
    "    title = title.strip()\n",
    "    description = description.strip()\n",
    "    if title[-1] not in '.!?': title += '.'\n",
    "    return f'{title} {description}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b491482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_prepare(examples):\n",
    "    result = {'key': [], 'caption': []}\n",
    "    for (key, title, description) in zip(examples['key'], examples['title_clean'], examples['description_clean']):\n",
    "        if image_is_valid(yfcc100m_images, key):\n",
    "            result['key'].append(key)\n",
    "            result['caption'].append(create_caption(title, description))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c35f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not yfcc100m_filtered.exists():\n",
    "    dataset = load_dataset('json', data_files=str(yfcc100m_metadata), split='train')\n",
    "    filtered = dataset.map(filter_and_prepare, batched=True, num_proc=num_workers, remove_columns=dataset.column_names)\n",
    "    print(f\"Using {len(filtered)} / {len(dataset)} items.\")\n",
    "    filtered.to_json(yfcc100m_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a74fbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-f98f54a1fa24c12b\n",
      "WARNING:datasets.builder:Reusing dataset json (/home/pedro/.cache/huggingface/datasets/json/default-f98f54a1fa24c12b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('json', data_files=str(yfcc100m_filtered), split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcca71a",
   "metadata": {},
   "source": [
    "## Torch Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5090cac4",
   "metadata": {},
   "source": [
    "We'll create a PyTorch Dataset and a DataLoader for convenience. This allows us to resize images when we retrieve items, and easily take batches of our desired size. An alternative would be to use a data collator instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66df0f6c",
   "metadata": {},
   "source": [
    "This is our simple PyTorch `Dataset` implementation. It simply reads images and center crops them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a511264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from PIL import ImageFile\n",
    "\n",
    "class YFCCDataset(Dataset):\n",
    "    def __init__(self, *, images_root, dataset: Dataset, max_len=None):\n",
    "        self.images_root = Path(images_root)\n",
    "        self.dataset = dataset\n",
    "        self.size = min(len(self.dataset), max_len) if max_len is not None else len(self.dataset)\n",
    "    \n",
    "    def _get_raw_image(self, i):\n",
    "        key = self.dataset[i]['key']\n",
    "        image_path = (self.images_root/key[0:3]/key[3:6]/key).with_suffix(\".jpg\")\n",
    "        image = default_loader(image_path)\n",
    "        return image\n",
    "    \n",
    "    def _center_crop(self, image, max_size=256):\n",
    "        s = min(image.size)\n",
    "        r = max_size / s\n",
    "        s = (round(r * image.size[1]), round(r * image.size[0]))\n",
    "        image = TF.resize(image, s, interpolation=InterpolationMode.LANCZOS)\n",
    "        image = TF.center_crop(image, output_size=2 * [max_size])\n",
    "        image = torch.from_numpy(np.array(image, copy=True))\n",
    "        image = torch.unsqueeze(image, 0)\n",
    "        return image.numpy()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        image = self._get_raw_image(i)\n",
    "        image = self._center_crop(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51706972",
   "metadata": {},
   "source": [
    "## VQGAN-JAX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e44f736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vqgan_jax.modeling_flax_vqgan import VQModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2053a6",
   "metadata": {},
   "source": [
    "We'll use a VQGAN trained with Taming Transformers and converted to a JAX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ef6c9ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n"
     ]
    }
   ],
   "source": [
    "model = VQModel.from_pretrained(\"flax-community/vqgan_f16_16384\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad01c3",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af19b8e0",
   "metadata": {},
   "source": [
    "Encoding is really simple using `shard` to automatically distribute \"superbatches\" across devices, and `pmap`. This is all it takes to create our encoding function, that will be jitted on first use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b99f6718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training.common_utils import shard\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88f36d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.pmap, axis_name=\"batch\")\n",
    "def encode(batch):\n",
    "    # Not sure if we should `replicate` params, does not seem to have any effect\n",
    "    _, indices = model.encode(batch)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba28246",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2210705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def encode_captioned_dataset(dataset, output_dir, batch_size=32, num_workers=16, save_every=14):\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    num_tpus = jax.device_count()\n",
    "    bs = batch_size * num_tpus\n",
    "    torch_dataset = YFCCDataset(images_root=yfcc100m_images, dataset=dataset)\n",
    "    dataloader = DataLoader(torch_dataset, batch_size=bs, num_workers=num_workers)\n",
    "    \n",
    "    # Saving strategy:\n",
    "    # - Create a new file every so often to prevent excessive file seeking.\n",
    "    # - Save each batch after processing.\n",
    "    # - Keep the file open until we are done with it.\n",
    "    file = None        \n",
    "    iterations = len(dataset) // bs\n",
    "    iter_loader = iter(dataloader)\n",
    "    for n in tqdm(range(iterations)):\n",
    "        if (n % save_every == 0):\n",
    "            if file is not None:\n",
    "                file.close()\n",
    "            split_num = n // save_every\n",
    "            file = open(output_dir/f'split_{split_num:05x}.jsonl', 'w')\n",
    "        batch = next(iter_loader).numpy()\n",
    "        batch = shard(batch.squeeze())\n",
    "        encoded = encode(batch)\n",
    "        encoded = encoded.reshape(-1, encoded.shape[-1])\n",
    "\n",
    "        # Extract captions from the dataset\n",
    "        start_index = n * batch_size * num_tpus\n",
    "        end_index = (n+1) * batch_size * num_tpus\n",
    "        \n",
    "        # Watch out! getting the column and then indexing is much slower than getting the slice first and then accessing the column\n",
    "        keys = dataset[start_index:end_index][\"key\"]\n",
    "        captions = dataset[start_index:end_index][\"caption\"]\n",
    "        \n",
    "        encoded_as_string = list(map(lambda item: np.array2string(item, separator=',', max_line_width=50000, formatter={'int':lambda x: str(x)}), encoded))\n",
    "        batch_df = pd.DataFrame.from_dict({\"key\": keys, \"caption\": captions, \"encoding\": encoded_as_string})\n",
    "        batch_df.to_json(file, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5567f0",
   "metadata": {},
   "source": [
    "Create a new file every 318 iterations. This should produce splits of ~500 MB each, when using a total batch size of 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04f1d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_every = 318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7704863d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 14462/14462 [3:56:55<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "encode_captioned_dataset(dataset, yfcc100m_output, batch_size=batch_size, num_workers=num_workers, save_every=save_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8953dd84",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db471c52d602b4f5f40ecaf278e88ccfef85c29d0a1a07185b0d51fc7acf4e26"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
